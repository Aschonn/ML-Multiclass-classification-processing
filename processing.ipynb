{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aschonn/anaconda3/envs/jupyter/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have no idea who I'm picking to go all the way ðŸ˜‰ My @yahoofantasy Tourney Pickâ€™em challenge isâ€¦  https://t.co/rTwRiDTOjQ\n",
      "1.9073486328125e-06\n"
     ]
    }
   ],
   "source": [
    "#generic imports\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "#Custom Imports\n",
    "\n",
    "from CMstyle import *\n",
    "from functions import *\n",
    "\n",
    "\n",
    "#custom inlines\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Import Raw Data\n",
    "\n",
    "df = pd.read_csv('original.csv')  \n",
    "\n",
    "#import Spell_check data\n",
    "\n",
    "spell_df = pd.read_csv('spell_check.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>money23green</td>\n",
       "      <td>I'm thankful to be placed in a situation where...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kdtrey5</td>\n",
       "      <td>T @lovelyone88: @KDthunderup I want rocks mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>Me at The Goo's show!!  http://t.co/NgZLbO2q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tedcruz</td>\n",
       "      <td>The Kay Bailey Hutchison Convention Center in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>money23green</td>\n",
       "      <td>@SoFarAhead__ lol what u gone do here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       username                                              tweet\n",
       "0  money23green  I'm thankful to be placed in a situation where...\n",
       "1       kdtrey5  T @lovelyone88: @KDthunderup I want rocks mode...\n",
       "2      ddlovato       Me at The Goo's show!!  http://t.co/NgZLbO2q\n",
       "3       tedcruz  The Kay Bailey Hutchison Convention Center in ...\n",
       "4  money23green              @SoFarAhead__ lol what u gone do here"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomize Order Of Data\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = shuffle(df)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "spell_df = shuffle(spell_df)\n",
    "spell_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "spell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ddlovato</td>\n",
       "      <td>Or this one.....?!!   http://say.ly/WGsnER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elonmusk</td>\n",
       "      <td>@CatherineUllo Glass corset hand-painted in pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>money23green</td>\n",
       "      <td>@Tm4_Bam @10mjones lol don't I always beat y'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elonmusk</td>\n",
       "      <td>It can transform into a robot, fight aliens an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>money23green</td>\n",
       "      <td>@kuddib lmao u OC bro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       username                                              tweet\n",
       "0      ddlovato         Or this one.....?!!   http://say.ly/WGsnER\n",
       "1      elonmusk  @CatherineUllo Glass corset hand-painted in pe...\n",
       "2  money23green  @Tm4_Bam @10mjones lol don't I always beat y'a...\n",
       "3      elonmusk  It can transform into a robot, fight aliens an...\n",
       "4  money23green                              @kuddib lmao u OC bro"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ddlovato', 'elonmusk', 'money23green', 'kdtrey5', 'tedcruz']\n"
     ]
    }
   ],
   "source": [
    "#OUR USERS\n",
    "\n",
    "my_tags = []\n",
    "for item in df['username']:\n",
    "    if item not in my_tags:\n",
    "        my_tags.append(item)\n",
    "print(my_tags)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['username' 'tweet']\n"
     ]
    }
   ],
   "source": [
    "#Our Features\n",
    "\n",
    "data_features = df.columns.values\n",
    "print(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Imports For Plots\n",
    "\n",
    "from pretty_matrix import *\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "my_tags = ['tedcruz', 'elonmusk', 'kdtrey5', 'ddlovato', 'money23green']\n",
    "\n",
    "\n",
    "class Plots:\n",
    "    \n",
    "    \n",
    "    def __init__(self, algorithm, functionName):\n",
    "    \n",
    "        self.algo = algorithm\n",
    "        self.functionName = functionName\n",
    "        \n",
    "        \n",
    "        \n",
    "    def createFolders(self, folder):\n",
    "        \n",
    "        if not os.path.isdir(folder):\n",
    "            os.mkdir(folder)\n",
    "                        \n",
    "        if not os.path.isdir(folder + '/' + self.algo):\n",
    "            os.mkdir(folder + '/' + self.algo)\n",
    "\n",
    "        \n",
    "      \n",
    "    def picOfConfusion(self, pipeline, y_test, y_pred):\n",
    "\n",
    "        \n",
    "        folder = \"confusionPics\"\n",
    "        filename = folder + '/' + self.algo + '/' + self.functionName + '_CM.png'\n",
    "        \n",
    "        self.createFolders(folder)\n",
    "        \n",
    "        #import download from \n",
    "        # https://github.com/wcipriano/pretty-print-confusion-matrix/blob/master/confusion_matrix_pretty_print.py\n",
    "        \n",
    "        plot_confusion_matrix_from_data(y_test, \n",
    "                                        y_pred, \n",
    "                                        columns = my_tags, \n",
    "                                        title = self.algo,\n",
    "                                        functionName = self.functionName\n",
    "                                       )\n",
    "\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "    def classification_report_plot(self, clf_report, functionName):\n",
    "                \n",
    "        folder = \"classificationPics\"\n",
    "        filename = folder + '/' + self.algo + '/' + self.functionName + '_CM.png'\n",
    "        \n",
    "        self.createFolders(folder)\n",
    "        \n",
    "        sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"YlGnBu\",  fmt='.3g')\n",
    "        \n",
    "        plt.title('Classification: ' + self.algo + ' ' + functionName)\n",
    "        \n",
    "        plt.savefig(filename, dpi=100)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports around class\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#RESULTS AFTER COMPLETION LOCATION\n",
    "\n",
    "BASE = dict()\n",
    "NB_results = dict()\n",
    "L_SVC_results = dict()\n",
    "LR_results = dict()\n",
    "KNN_results = dict()\n",
    "DT_results = dict()\n",
    "RF_results = dict()\n",
    "xgboost_results = dict()\n",
    "\n",
    "def preprocess(dataset, testSize, random_state = 0):\n",
    "\n",
    "    X = df.tweet\n",
    "    y = df.username\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testSize, random_state=32)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def alterDataByFunction(dataset, function):\n",
    "    \n",
    "    dataset['tweet'] = dataset['tweet'].apply(lambda x: function(x))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "class Algorithms:\n",
    "      \n",
    "    def __init__(self, dataset, functionName):\n",
    "        \n",
    "        self.functionName = functionName.__name__\n",
    "        \n",
    "        if self.functionName == 'spell_check':\n",
    "            self.altereddataset = spell_df\n",
    "        else:\n",
    "            self.altereddataset = dataset\n",
    "\n",
    "        #Show the data is changing\n",
    "        \n",
    "        print(self.functionName)\n",
    "        print(dataset['tweet'][0:5])\n",
    "        \n",
    "        #Split in training and testing sets\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = preprocess(self.altereddataset, 0.3, random_state = 32)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "##############################################################################################################\n",
    "        \n",
    "    def NB(self):\n",
    "        \n",
    "        #Import Classifier\n",
    "        \n",
    "        from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "\n",
    "        accuracy = 0 \n",
    "        speed = 0\n",
    "        percision = 0\n",
    "        f1 = 0\n",
    "        recall = 0\n",
    "        \n",
    "        for count in range(5):\n",
    "\n",
    "            nb = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf', MultinomialNB()),\n",
    "                       ])\n",
    "                        \n",
    "        # Calculate ROC_AUC\n",
    "        # There are two options for average: macro or weighted\n",
    "        # We are going with Macro since we have a balanced dataset\n",
    "            \n",
    "            nb.fit(self.X_train, self.y_train)\n",
    "            y_pred = nb.predict(self.X_test)\n",
    "            accuracy += accuracy_score(y_pred, self.y_test)\n",
    "            percision += precision_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            f1 += f1_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            recall += recall_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "        \n",
    "        #create confusion matrix and classification files (Set for only base)\n",
    "        \n",
    "        if self.functionName == 'base' or self.functionName == 'rm_punctuation':\n",
    "\n",
    "            algorithm = 'Naive Bayes'\n",
    "            plots = Plots(algorithm, self.functionName)\n",
    "            plots.picOfConfusion(nb, self.y_test, y_pred)\n",
    "            plots.classification_report_plot(classification_report(self.y_test, y_pred, output_dict=True),self.functionName)\n",
    "\n",
    "        #Puts accuracy and speed into dictionary for later\n",
    "#         accuracy/5, speed/5, percision/5, f1/5, recall/5, roc_score/5\n",
    "                                \n",
    "        NB_results[self.functionName] = [accuracy/5, speed/5, percision/5, f1/5, recall/5]\n",
    "\n",
    "#####################################################################################################################\n",
    "        \n",
    "    def L_SVC(self):\n",
    "        \n",
    "        #Import Classifier\n",
    "        \n",
    "        from sklearn.svm import LinearSVC\n",
    "        \n",
    "        accuracy = 0 \n",
    "        speed = 0\n",
    "        percision = 0\n",
    "        f1 = 0\n",
    "        recall = 0\n",
    "        \n",
    "        #Runs through algorithm 5 times to get an average accuracy\n",
    "        \n",
    "        for count in range(5):\n",
    "            \n",
    "            print('L_scv', count)\n",
    "\n",
    "            nb = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf', LinearSVC()),\n",
    "                       ])\n",
    "            \n",
    "                        \n",
    "            # Calculate ROC_AUC\n",
    "            # There are two options for average: macro or weighted\n",
    "            # We are going with Macro since we have a balanced dataset\n",
    "            \n",
    "            nb.fit(self.X_train, self.y_train)\n",
    "            y_pred = nb.predict(self.X_test)\n",
    "            accuracy += accuracy_score(y_pred, self.y_test)\n",
    "            percision += precision_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            f1 += f1_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            recall += recall_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            \n",
    "        #create confusion matrix and classification files (Set for only base)\n",
    "        \n",
    "        if self.functionName == 'base' or self.functionName == 'rm_punctuation':\n",
    "\n",
    "            algorithm = 'L_SVC'\n",
    "            plots = Plots(algorithm, self.functionName)\n",
    "            plots.picOfConfusion(nb, self.y_test, y_pred)\n",
    "            plots.classification_report_plot(classification_report(self.y_test, y_pred, output_dict=True),self.functionName)\n",
    "\n",
    "        #Puts accuracy and speed into dictionary for later\n",
    "#         accuracy/5, speed/5, percision/5, f1/5, recall/5, roc_score/5\n",
    "                                \n",
    "        L_SVC_results[self.functionName] = [accuracy/5, speed/5, percision/5, f1/5, recall/5]\n",
    "        \n",
    "#######################################################################################################################   \n",
    "    \n",
    "    def DT(self):\n",
    "        \n",
    "        #Import Classifier\n",
    "\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        \n",
    "        \n",
    "        accuracy = 0 \n",
    "        speed = 0\n",
    "        percision = 0\n",
    "        f1 = 0\n",
    "        recall = 0\n",
    "        \n",
    "        #Runs through algorithm 5 times to get an average accuracy\n",
    "        \n",
    "        for count in range(5):\n",
    "            \n",
    "            print('DT', count)\n",
    "\n",
    "            nb = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf', DecisionTreeClassifier()),\n",
    "                       ])\n",
    "            \n",
    "                        \n",
    "            # Calculate ROC_AUC\n",
    "            # There are two options for average: macro or weighted\n",
    "            # We are going with Macro since we have a balanced dataset\n",
    "            \n",
    "            nb.fit(self.X_train, self.y_train)\n",
    "            y_pred = nb.predict(self.X_test)\n",
    "            accuracy += accuracy_score(y_pred, self.y_test)\n",
    "            percision += precision_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            f1 += f1_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            recall += recall_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            \n",
    "        #create confusion matrix and classification files (Set for only base)\n",
    "        \n",
    "        if self.functionName == 'base' or self.functionName == 'rm_punctuation':\n",
    "\n",
    "            algorithm = 'Decision Tree'\n",
    "            plots = Plots(algorithm, self.functionName)\n",
    "            plots.picOfConfusion(nb, self.y_test, y_pred)\n",
    "            plots.classification_report_plot(classification_report(self.y_test, y_pred, output_dict=True),self.functionName)\n",
    "\n",
    "        #Puts accuracy and speed into dictionary for later\n",
    "#         accuracy/5, speed/5, percision/5, f1/5, recall/5, roc_score/5\n",
    "                                \n",
    "        DT_results[self.functionName] = [accuracy/5, speed/5, percision/5, f1/5, recall/5]\n",
    "        \n",
    "##########################################################################################################################\n",
    "\n",
    "    def RF(self):\n",
    "        \n",
    "        #Import Classifier\n",
    "\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        \n",
    "        accuracy = 0 \n",
    "        speed = 0\n",
    "        percision = 0\n",
    "        f1 = 0\n",
    "        recall = 0\n",
    "        \n",
    "        #Runs through algorithm 5 times to get an average accuracy\n",
    "        \n",
    "        for count in range(5):\n",
    "            \n",
    "            print('RF', count)\n",
    "\n",
    "            nb = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf', RandomForestClassifier()),\n",
    "                       ])\n",
    "            \n",
    "                        \n",
    "            # Calculate ROC_AUC\n",
    "            # There are two options for average: macro or weighted\n",
    "            # We are going with Macro since we have a balanced dataset\n",
    "            \n",
    "            nb.fit(self.X_train, self.y_train)\n",
    "            y_pred = nb.predict(self.X_test)\n",
    "            accuracy += accuracy_score(y_pred, self.y_test)\n",
    "            percision += precision_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            f1 += f1_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            recall += recall_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            \n",
    "        #create confusion matrix and classification files (Set for only base)\n",
    "        \n",
    "        if self.functionName == 'base' or self.functionName == 'rm_punctuation':\n",
    "\n",
    "            algorithm = 'Random Forest'\n",
    "            plots = Plots(algorithm, self.functionName)\n",
    "            plots.picOfConfusion(nb, self.y_test, y_pred)\n",
    "            plots.classification_report_plot(classification_report(self.y_test, y_pred, output_dict=True),self.functionName)\n",
    "\n",
    "        #Puts accuracy and speed into dictionary for later\n",
    "#         accuracy/5, speed/5, percision/5, f1/5, recall/5, roc_score/5\n",
    "        RF_results[self.functionName] = [accuracy/5, speed/5, percision/5, f1/5, recall/5]\n",
    "#################################################################################################################     \n",
    "        \n",
    "    def KNN(self):\n",
    "        \n",
    "        #Import Classifier\n",
    "\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "        accuracy = 0 \n",
    "        speed = 0\n",
    "        percision = 0\n",
    "        f1 = 0\n",
    "        recall = 0\n",
    "        \n",
    "        #Runs through algorithm 5 times to get an average accuracy\n",
    "        \n",
    "        for count in range(5):\n",
    "            \n",
    "            print('KNN ', count)\n",
    "\n",
    "            nb = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf', KNeighborsClassifier()),\n",
    "                       ])\n",
    "            \n",
    "                        \n",
    "            # Calculate ROC_AUC\n",
    "            # There are two options for average: macro or weighted\n",
    "            # We are going with Macro since we have a balanced dataset\n",
    "            \n",
    "            nb.fit(self.X_train, self.y_train)\n",
    "            y_pred = nb.predict(self.X_test)\n",
    "            accuracy += accuracy_score(y_pred, self.y_test)\n",
    "            percision += precision_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            f1 += f1_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            recall += recall_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            \n",
    "        #create confusion matrix and classification files (Set for only base)\n",
    "        \n",
    "        if self.functionName == 'base':\n",
    "        \n",
    "            algorithm = 'K-Nearest Neighbor'\n",
    "\n",
    "            plots = Plots(algorithm, self.functionName)\n",
    "            plots.picOfConfusion(nb, self.y_test, y_pred)\n",
    "            plots.classification_report_plot(classification_report(self.y_test, y_pred, output_dict=True), self.functionName)\n",
    "\n",
    "        #Puts accuracy and speed into dictionary for later\n",
    "        \n",
    "        KNN_results[self.functionName] = [accuracy/5, speed/5, percision/5, f1/5, recall/5]\n",
    "\n",
    "###################################################################################################################\n",
    "\n",
    "    def LR(self):\n",
    "        \n",
    "        #Import Classifier\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        \n",
    "        accuracy = 0 \n",
    "        speed = 0\n",
    "        percision = 0\n",
    "        f1 = 0\n",
    "        recall = 0\n",
    "        \n",
    "        #Runs through algorithm 5 times to get an average accuracy\n",
    "        \n",
    "        for count in range(5):\n",
    "            \n",
    "            print('LR ', count)\n",
    "\n",
    "            nb = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf', LogisticRegression(C=100,random_state=0,max_iter=10000, multi_class='multinomial'))\n",
    "                       ])\n",
    "            \n",
    "                        \n",
    "            # Calculate ROC_AUC\n",
    "            # There are two options for average: macro or weighted\n",
    "            # We are going with Macro since we have a balanced dataset\n",
    "            \n",
    "            nb.fit(self.X_train, self.y_train)\n",
    "            y_pred = nb.predict(self.X_test)\n",
    "            accuracy += accuracy_score(y_pred, self.y_test)\n",
    "            percision += precision_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            f1 += f1_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            recall += recall_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            \n",
    "        #create confusion matrix and classification files (Using only base for demonstration purposes, but can be used for all if needed)\n",
    "        \n",
    "        if self.functionName == 'base':\n",
    "        \n",
    "            algorithm = 'Logistic Regression'\n",
    "\n",
    "            plots = Plots(algorithm, self.functionName)\n",
    "            plots.picOfConfusion(nb, self.y_test, y_pred)\n",
    "            plots.classification_report_plot(classification_report(self.y_test, y_pred, output_dict=True), self.functionName)            \n",
    "        \n",
    "        #Puts accuracy and speed into dictionary for later\n",
    "       \n",
    "        LR_results[self.functionName] = [accuracy/5, speed/5, percision/5, f1/5, recall/5]\n",
    "\n",
    "\n",
    "#####################################################################################################################\n",
    "        \n",
    "    def xgboost(self):\n",
    "        \n",
    "        #import classifier\n",
    "\n",
    "        from xgboost import XGBClassifier\n",
    "        \n",
    "        accuracy = 0 \n",
    "        speed = 0\n",
    "        percision = 0\n",
    "        f1 = 0\n",
    "        recall = 0\n",
    "        \n",
    "        #Runs through algorithm 5 times to get an average accuracy\n",
    "        \n",
    "        for count in range(5):\n",
    "            \n",
    "            print('XG ', count)\n",
    "\n",
    "            nb = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf', XGBClassifier(objective='multi:softmax', num_class=5)),\n",
    "                       ])\n",
    "            \n",
    "                        \n",
    "            # Calculate ROC_AUC\n",
    "            # There are two options for average: macro or weighted\n",
    "            # We are going with Macro since we have a balanced dataset\n",
    "            \n",
    "            nb.fit(self.X_train, self.y_train)\n",
    "            y_pred = nb.predict(self.X_test)\n",
    "            accuracy += accuracy_score(y_pred, self.y_test)\n",
    "            percision += precision_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            f1 += f1_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            recall += recall_score(self.y_test, y_pred, labels = my_tags, average=\"macro\")\n",
    "            \n",
    "\n",
    "        #create confusion matrix and classification files\n",
    "        \n",
    "        if self.functionName == 'base':\n",
    "        \n",
    "            algorithm = 'XGBoost'\n",
    "\n",
    "            plots = Plots(algorithm, self.functionName)\n",
    "            plots.picOfConfusion(nb, self.y_test, y_pred)\n",
    "            plots.classification_report_plot(classification_report(self.y_test, y_pred, output_dict=True),self.functionName)\n",
    "        \n",
    "        #Puts accuracy and speed into dictionary for later\n",
    "        \n",
    "        xgboost_results[self.functionName] = [accuracy/5, speed/5, percision/5, f1/5, recall/5]\n",
    "\n",
    "##########################################################################################################\n",
    "    \n",
    "    def runall(self):\n",
    "        \n",
    "    #Uncomment To Use:\n",
    "        \n",
    "        #Round 1\n",
    "        \n",
    "#         self.DT()\n",
    "#         self.RF()\n",
    "#         self.L_SVC()\n",
    "#         self.NB()\n",
    "        \n",
    "        #Round 2\n",
    "        \n",
    "#         self.xgboost()\n",
    "#         self.KNN()\n",
    "        self.LR()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting:  base\n",
      "base\n",
      "0           Or this one.....?!!   http://say.ly/WGsnER\n",
      "1    @CatherineUllo Glass corset hand-painted in pe...\n",
      "2    @Tm4_Bam @10mjones lol don't I always beat y'a...\n",
      "3    It can transform into a robot, fight aliens an...\n",
      "4                                @kuddib lmao u OC bro\n",
      "Name: tweet, dtype: object\n",
      "LR  0\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "import time\n",
    "# !pip install autocorrect\n",
    "\n",
    "functions = [\n",
    "        base,\n",
    "        rm_punctuation,\n",
    "        lemmatize,\n",
    "        rm_stopwords,\n",
    "        rm_numbers,\n",
    "        stemming,\n",
    "        rp_elongated,\n",
    "        rm_noise,\n",
    "        rp_twitterHandles,\n",
    "        contraction,\n",
    "        rp_slang,\n",
    "        spell_check,\n",
    "]\n",
    "\n",
    "def tester(df):\n",
    "    \n",
    "    for function in functions:\n",
    "        \n",
    "        print('Starting: ', function.__name__)\n",
    "        \n",
    "        #Sends a copy of our altered dataframe to our class algorithms and runs each algorithm and places the results in the dictionaries above\n",
    "        Algorithms(alterDataByFunction(df.copy(), function), function).runall()\n",
    "    \n",
    "    \n",
    "tester(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Out Results\n",
    "\n",
    "#round 1\n",
    "\n",
    "print(NB_results)\n",
    "# print(L_SVC_results)\n",
    "# print(DT_results)\n",
    "# print(RF_results)\n",
    "\n",
    "#round 2\n",
    "\n",
    "print(LR_results)\n",
    "# print(KNN_results)\n",
    "# print(xgboost_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_orders = sorted(NB_results.items(), key=lambda x: x[1][1], reverse=True)  \n",
    "for i in sort_orders:\n",
    "    print(i[0], i[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def techniquePlot(dictionary, name, metric):\n",
    "    \n",
    "    if metric.lower() == 'accuracy':\n",
    "        scoreName = 'Accuracy'\n",
    "        scoreIndex = 0\n",
    "        \n",
    "    #index1 is for speed but it's not ready yet\n",
    "    if metric.lower() == 'speed':\n",
    "        scoreName = 'Speed'\n",
    "        scoreIndex = 1\n",
    "    \n",
    "    if metric.lower() == 'precision':\n",
    "        scoreName = 'Precision'\n",
    "        scoreIndex = 2\n",
    "    if metric.lower() == 'f1':\n",
    "        scoreName = 'F1'\n",
    "        scoreIndex = 3\n",
    "    if metric.lower() == 'recall':\n",
    "        scoreName = 'Recall'\n",
    "        scoreIndex = 4\n",
    "\n",
    "    \n",
    "    \n",
    "    #folder storage\n",
    "\n",
    "    folder = 'Technique' + scoreName + 'Plot'\n",
    "    filename = folder + '/' + name + scoreName + '.png'\n",
    "\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    techniques=[]\n",
    "    accuracy =['Base', scoreName]\n",
    "    bar_width = .20\n",
    "    baseline = []\n",
    "    preformance=[]\n",
    "\n",
    "    for key, value in dictionary.items():\n",
    "\n",
    "        techniques.append(key)\n",
    "        preformance.append(value[scoreIndex] * 100)\n",
    "\n",
    "    for count in range(len(preformance)):\n",
    "\n",
    "        baseline.append(preformance[scoreIndex])\n",
    "    \n",
    "\n",
    "    pos = np.arange(len(techniques))\n",
    "\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['axes.axisbelow'] = True\n",
    "    plt.figure(figsize=(25,10))\n",
    "    plt.grid(axis = 'y', zorder=0)\n",
    "    plt.bar(pos, baseline, bar_width,color='blue',edgecolor='black')\n",
    "    plt.bar(pos + bar_width, preformance, bar_width, color='pink', edgecolor='black')\n",
    "    plt.xticks(pos, techniques)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.ylim([min(preformance) - .05, max(preformance) + .05])\n",
    "    plt.legend(accuracy, fontsize = 20, loc='upper left', frameon=False)\n",
    "    plt.xlabel('Techniques', fontsize=25)\n",
    "    plt.ylabel(scoreName + ' (%)', fontsize=25)\n",
    "    plt.title('Technique ' + scoreName + ' Comparison ' + '- ' + name, fontsize=18)\n",
    "    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Uncomment To Use:\n",
    "\n",
    "#round 1\n",
    "\n",
    "# techniqueComparisonPlot(NB_results, 'Naive Bayes')\n",
    "\n",
    "scores = ['accuracy', 'recall', 'precision', 'f1']\n",
    "\n",
    "#linear support vector machine\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "#     techniquePlot(L_SVC_results, 'Linear Support vector machines', score)\n",
    "#     techniquePlot(DT_results, 'Decision Tree', score)\n",
    "#     techniquePlot(RF_results, 'Random Forest', score)\n",
    "\n",
    "#     techniquePlot(NB_results, 'Naive Bayes', score)\n",
    "\n",
    "#     techniquePlot(KNN_results, 'KNN', score)\n",
    "#     techniquePlot(xgboost_results, 'XGBoost', score)\n",
    "    techniquePlot(LR_results, 'Logistic Regression', score)\n",
    "\n",
    "\n",
    "# #round 2\n",
    "\n",
    "# techniqueComparisonPlot(KNN_results, 'KNN')\n",
    "# techniqueComparisonPlot(xgboost_results, 'XGBoost')\n",
    "# techniqueComparisonPlot(LR_results, 'Logistic Regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
